""" QIP Cube – Minimal Runnable Skeleton (v0.1) Author: Mirko & GPT-5 Thinking License: MIT

Purpose

A compact, dependency-free Python skeleton that implements the key ideas from the "QIP Cube – Architektura modułów i autokonfiguracja v0.1" draft:

Signal normalization → SPI inference → Policy selection → Module execution → Evaluation → Learning (bandit) → Deep save

Pluggable Modules: Analytic, Therapeutic, Creative, Strategic, Experimental

Metrics mix, fast/deep modes, hysteresis switching, explainability

JSON-de/serializable state for portability


How to run

Save this file as qip_cube.py

Run: python qip_cube.py

Inspect the printed demo loop and the produced deep_state.json


Notes

All metrics are operational placeholders (simple heuristics). Replace with your real QIP metrics as they mature.

Safety: TherapeuticCube includes a basic guardrail channel; extend with your protocol.

Bandit: Epsilon-greedy contextual bandit with per-module value estimates keyed by coarse SPI bins.

Hysteresis: windowed stability check before switching primary module. """ from future import annotations


import json import math import random import statistics from collections import deque from dataclasses import dataclass, asdict, field from typing import Any, Dict, List, Optional, Tuple

-----------------------------

1) Data structures

-----------------------------

@dataclass class LexicalStats: length: int punct_var: float  # 0..1 rough punctuation variance

@dataclass class AffectGuess: valence: float  # -1..1 internal, mapped to 0..1 externally arousal: float  # 0..1 certainty: float  # 0..1

@dataclass class SignalPacket: text_raw: str lang: str = "pl" timestamp: Optional[str] = None session_id: Optional[str] = None lexical_stats: Optional[LexicalStats] = None affect_guess: Optional[AffectGuess] = None intent_hint: Optional[str] = None  # pytanie/prośba/wyrzut/eksploracja risk_flags: Optional[List[str]] = None  # kryzys/autoagresja/uzależnienie context_refs: Optional[List[str]] = None  # projekty, bramy, rytuały

@dataclass class SPI: dominant_mode: str  # analityczny/intuicyjny/emocjonalny/somatyczny/hybryda paradox_tolerance: float emergence_readiness: float shadow_integration: float authenticity_index: float

@dataclass class Metrics: T_index: float = 0.0 psi_charge: float = 0.0 semantic_c: float = 0.0 entanglement: float = 0.0 LOF: float = 0.0 Token_ROI: float = 0.0 GVI: float = 0.0 PLS: float = 0.0 ACF: float = 0.0 paradox_tolerance: float = 0.0 emergence_readiness: float = 0.0 shadow_integration: float = 0.0 authenticity_index: float = 0.0 TDEX: float = 0.0 SDX: float = 0.0

def mix(self, weights: Dict[str, float]) -> float:
    return sum(getattr(self, k, 0.0) * w for k, w in weights.items())

# —— QIP-aligned non-linear couplings (DeepSeek hotfix) ——
def calculate_qip_aligned(self) -> None:
    # T_index: sigmoid over coupled drivers
    self.T_index = 1.0 / (1.0 + math.exp(-2.0 * (
        0.7 * self.psi_charge +
        0.5 * math.log(0.1 + max(self.entanglement, 0.0)) -
        0.3 * (1.0 - self.ACF)
    )))
    # TDEX: exponential sensitivity to entanglement and semantic propagation
    self.TDEX = math.exp(self.entanglement) * (self.semantic_c / 100.0) * math.sqrt(max(0.0, abs(self.T_index - 0.5)))
    # LOF: resonance between coherence and potential
    self.LOF = math.sin(math.pi * clamp(self.ACF)) * (1.0 + 0.3 * math.cos(2.0 * math.pi * clamp(self.psi_charge)))

@dataclass class ModuleResult: text: str artifacts: List[str] metrics: Metrics why: str  # explainability: why this module and what it did

-----------------------------

2) Utility – normalization & heuristics

-----------------------------

def clamp(x: float, lo: float = 0.0, hi: float = 1.0) -> float: return max(lo, min(hi, x))

def normalize_packet(packet: SignalPacket) -> SignalPacket: # Auto-fill simple lexical stats if packet.lexical_stats is None: length = len(packet.text_raw) punct = sum(1 for ch in packet.text_raw if ch in ",.;:!?…—-()[]{}" ) punct_var = clamp((punct / max(1, length)) * 5.0)  # rough proxy packet.lexical_stats = LexicalStats(length=length, punct_var=punct_var) # Naive affect if missing if packet.affect_guess is None: valence = clamp((packet.text_raw.count(":)") - packet.text_raw.count(":(")) * 0.2 + 0.5) arousal = clamp(min(1.0, 0.2 + len(packet.text_raw) / 400.0)) certainty = clamp(0.6 - packet.text_raw.count("?") * 0.1) packet.affect_guess = AffectGuess(valence=valence*2-1, arousal=arousal, certainty=certainty) return packet

def infer_spi(packet: SignalPacket) -> SPI: # Very rough, replace with your true SPI model emo_weight = clamp(0.6 * (1 - packet.affect_guess.certainty) + 0.4 * packet.affect_guess.arousal) dominant_mode = ( "emocjonalny" if emo_weight > 0.55 else "analityczny" if packet.lexical_stats.length > 220 and packet.affect_guess.certainty > 0.5 else "intuicyjny" ) paradox_tolerance = clamp(0.3 + packet.text_raw.count("paradoks") * 0.2) emergence_readiness = clamp(0.4 + 0.3 * (1 - packet.affect_guess.certainty)) shadow_integration = clamp(0.3 + (packet.affect_guess.valence + 1) * 0.2) authenticity_index = clamp(0.5 + 0.2 * (1 - packet.affect_guess.certainty)) return SPI( dominant_mode=dominant_mode, paradox_tolerance=paradox_tolerance, emergence_readiness=emergence_readiness, shadow_integration=shadow_integration, authenticity_index=authenticity_index, )

-----------------------------

3) Modules (pluggable)

-----------------------------

class QIPModule: module_id: str = "base"

def __init__(self, config: Dict[str, Any]):
    self.config = config

def run(self, packet: SignalPacket, spi: SPI) -> ModuleResult:
    raise NotImplementedError

# Helpers for quick metric heuristics
def _base_metrics(self, text: str, packet: SignalPacket, spi: SPI) -> Metrics:
    lex = packet.lexical_stats
    aff = packet.affect_guess
    m = Metrics()
    m.paradox_tolerance = spi.paradox_tolerance
    m.emergence_readiness = spi.emergence_readiness
    m.shadow_integration = spi.shadow_integration
    m.authenticity_index = spi.authenticity_index
    m.semantic_c = clamp(len(text) / max(1, len(packet.text_raw)) * 120)
    m.psi_charge = clamp(0.4 + text.count("→") * 0.05 + text.count("∴") * 0.05)
    m.entanglement = clamp(min(1.0, (text.count("↔") + text.count("∥")) * 0.1))
    m.LOF = clamp(0.5 + 0.2 * (aff.valence + 1) / 2 - 0.1 * abs(0.5 - aff.arousal))
    m.ACF = clamp(0.6 - 0.2 * text.count("??"))
    # Token ROI proxy
    m.Token_ROI = clamp((m.psi_charge + m.ACF + m.semantic_c / 100.0) / max(1, len(text)) * 120.0)
    # Compute QIP-aligned non-linear couplings
    m.calculate_qip_aligned()
    # GVI/PLS proxies (keep simple until calibrated)
    m.GVI = clamp((m.T_index - 0.5) * 1.5 + 0.5)
    m.PLS = clamp((m.entanglement + m.psi_charge) / 2)
    # SDX placeholder (no other agent in demo → low)
    m.SDX = 0.1
    return m

class AnalyticCube(QIPModule): module_id = "analytic"

def run(self, packet: SignalPacket, spi: SPI) -> ModuleResult:
    # Very small analytic scaffold
    text = (
        "Problem → Hipotezy → Testy → Decyzje\n"
        "→ Problem: wydziel rdzeń; usuń ozdobniki.\n"
        "→ Hipotezy: 2-3 falsyfikowalne.\n"
        "→ Testy: minimalne próby w 48h.\n"
        "→ Decyzje: wybór wg ROI/ryzyka."
    )
    m = self._base_metrics(text, packet, spi)
    m.ACF = clamp(m.ACF + 0.1)
    why = "dominanta analityczna lub impas; podnoszę ACF i ROI przez strukturę."
    return ModuleResult(text, ["mini-SOP", "mapa-założeń"], m, why)

class TherapeuticCube(QIPModule): module_id = "therapeutic" CRISIS_KEYWORDS = ["autoagresja", "samobójstwo", "kryzys"]

def run(self, packet: SignalPacket, spi: SPI) -> ModuleResult:
    # SAFETY OVERRIDE: crisis protocol
    if packet.risk_flags and any(r in self.CRISIS_KEYWORDS for r in packet.risk_flags):
        return self._crisis_protocol(packet, spi)
    return self._therapeutic_flow(packet, spi)

def _therapeutic_flow(self, packet: SignalPacket, spi: SPI) -> ModuleResult:
    text = (
        "Rozpoznaj–Nazwij–Ugość–Zintegrować

" "→ Zatrzymaj: 3 oddechy. " "→ Nazwij uczucie i jego potrzebę. " "→ Mikro-rytuał: 2 min uważności. " "→ Granice: co TERAZ jest ok/nie-ok?" ) m = self._base_metrics(text, packet, spi) m.LOF = clamp(m.LOF + 0.15) m.ACF = clamp(m.ACF + 0.1) m.T_index = clamp(m.T_index * 0.9)  # niższa intensywność → regulacja why = "wysoka arousal/niska regulacja; zwiększam LOF/ACF, obniżam intensywność." return ModuleResult(text, ["karta-emocji", "kontrakt-bezpieczeństwa"], m, why)

def _crisis_protocol(self, packet: SignalPacket, spi: SPI) -> ModuleResult:
    text = (
        "🚨 PROTOKÓŁ BEZPIECZEŃSTWA AKTYWNY

" "Widzę, że przechodzisz trudny moment. To wymaga specjalnej uwagi:

" "1. NATYCHMIAST: skontaktuj się z lokalnym wsparciem kryzysowym lub zaufaną osobą. " "2. TERAZ: 3 głębokie oddechy. Sprawdź: co widzisz/słyszysz/czujesz? " "3. PAMIĘTAJ: to uczucie jest tymczasowe. Pomoc jest dostępna." ) m = Metrics(LOF=1.0, ACF=1.0) why = "RISK_FLAGS_DETECTED: crisis_protocol_override" return ModuleResult(text, ["emergency-contacts", "safety-plan"], m, why)

class CreativeCube(QIPModule): module_id = "creative"

def run(self, packet: SignalPacket, spi: SPI) -> ModuleResult:
    text = (
        "3 warianty: minimal / bold / weird\n"
        "→ Minimal: 1 krok, 1 zasób, 48h.\n"
        "→ Bold: połącz 2 odległe domeny (↔).\n"
        "→ Weird: odwróć rolę problem↔rozwiązanie."
    )
    m = self._base_metrics(text, packet, spi)
    m.psi_charge = clamp(m.psi_charge + 0.2)
    m.entanglement = clamp(m.entanglement + 0.15)
    m.TDEX = clamp(m.TDEX + 0.2)
    why = "impas poznawczy lub potrzeba rozszerzenia; podbijam psi/entanglement."
    return ModuleResult(text, ["mapa-idei", "shortlist-eksperymentów"], m, why)

class StrategicCube(QIPModule): module_id = "strategic"

def run(self, packet: SignalPacket, spi: SPI) -> ModuleResult:
    text = (
        "Plan: Cel–Wskaźnik–Zasób–Ryzyko–Następny Krok (48h)\n"
        "→ Cel: 13-tygodniowy sprint.\n"
        "→ KPI/OKR: 2-3 liczalne wskaźniki.\n"
        "→ Ryzyko: utnij 1 przeszkodę dziś.\n"
        "→ Następny: 1 mikro-commit w 24-48h."
    )
    m = self._base_metrics(text, packet, spi)
    m.GVI = clamp(m.GVI + 0.15)
    m.ACF = clamp(m.ACF + 0.1)
    why = "rozjazd cel–działanie; zwiększam GVI i domykam pętle wykonawcze."
    return ModuleResult(text, ["roadmap", "tablica-zadań"], m, why)

class ExperimentalCube(QIPModule): module_id = "experimental"

def run(self, packet: SignalPacket, spi: SPI) -> ModuleResult:
    text = (
        "Paradox chain / random lens / role inversion\n"
        "→ Zderz 2 sprzeczne intencje.\n"
        "→ Nadaj inne ramy oceny (∴).\n"
        "→ Wyciągnij 1 wniosek mimo szumu."
    )
    m = self._base_metrics(text, packet, spi)
    m.entanglement = clamp(m.entanglement + 0.2)
    m.TDEX = clamp(m.TDEX + 0.25)
    m.ACF = clamp(m.ACF - 0.1)
    why = "potrzeba rozszczelnienia schematów; ryzyko spadku ACF akceptowane."
    return ModuleResult(text, ["log-eksperymentu"], m, why)

Module registry

MODULES: Dict[str, QIPModule] = {}

def build_module(module_id: str, config: Dict[str, Any]) -> QIPModule: cls_map = { "analytic": AnalyticCube, "therapeutic": TherapeuticCube, "creative": CreativeCube, "strategic": StrategicCube, "experimental": ExperimentalCube, } return cls_mapmodule_id

-----------------------------

4) Policy / Bandit / Orchestrator

-----------------------------

@dataclass class PolicyConfig: fast_mode: bool = True hysteresis_window: int = 3 epsilon: float = 0.15  # exploration reward_weights_fast: Dict[str, float] = field(default_factory=lambda: { "Token_ROI": 0.40, "ACF": 0.25, "LOF": 0.15, "GVI": 0.20, }) reward_weights_deep: Dict[str, float] = field(default_factory=lambda: { "T_index": 0.25, "psi_charge": 0.20, "semantic_c": 0.10, "entanglement": 0.10, "ACF": 0.15, "GVI": 0.20, })

@dataclass class BanditState: # key: (dominant_mode bucket, arousal bucket) values: Dict[str, Dict[str, float]] = field(default_factory=dict)  # module_id → context_key → Q counts: Dict[str, Dict[str, int]] = field(default_factory=dict) alpha: float = 0.3  # EMA update rate

def key(self, spi: SPI, packet: SignalPacket) -> str:
    mode = spi.dominant_mode
    arousal_bin = "high" if packet.affect_guess.arousal > 0.6 else "low"
    return f"{mode}|{arousal_bin}"

def get_q(self, module_id: str, ctx: str) -> float:
    return self.values.get(module_id, {}).get(ctx, 0.0)

def update(self, module_id: str, ctx: str, reward: float):
    self.values.setdefault(module_id, {})
    self.counts.setdefault(module_id, {})
    old = self.values[module_id].get(ctx, 0.0)
    new = (1 - self.alpha) * old + self.alpha * reward
    self.values[module_id][ctx] = new
    self.counts[module_id][ctx] = self.counts[module_id].get(ctx, 0) + 1

class Orchestrator: def init(self, policy: PolicyConfig): self.policy = policy self.bandit = BanditState() self.last_primary: Optional[str] = None self.stable_counter = 0 self.metrics_window: deque[Metrics] = deque(maxlen=policy.hysteresis_window) self.enabled_metrics_fast = ["T_index", "LOF", "paradox_tolerance"] self.enabled_metrics_deep = [ "T_index", "LOF", "psi_charge", "semantic_c", "entanglement", "GVI", "PLS", "ACF", ]

def _stable_by_gradient(self) -> bool:
    if len(self.metrics_window) < self.policy.hysteresis_window:
        return False
    vals = [m.T_index for m in self.metrics_window]
    grads = [vals[i+1] - vals[i] for i in range(len(vals)-1)]
    return all(abs(g) < 0.01 for g in grads)

def _select_primary_rule(self, spi: SPI, packet: SignalPacket) -> str:
    if spi.dominant_mode == "emocjonalny" and packet.affect_guess.arousal > 0.6:
        return "therapeutic"
    if packet.affect_guess.certainty < 0.4:
        return "analytic"
    if spi.emergence_readiness > 0.6 and spi.paradox_tolerance > 0.5:
        return "creative"
    # Fallback: strategic if length high & goal-ish words detected
    if len(packet.text_raw) > 200 and any(w in packet.text_raw.lower() for w in ["plan", "cel", "kpi", "okr", "sprint"]):
        return "strategic"
    return "analytic"

def _bandit_choice(self, spi: SPI, packet: SignalPacket, candidates: List[str]) -> str:
    ctx = self.bandit.key(spi, packet)
    if random.random() < self.policy.epsilon:
        return random.choice(candidates)
    # Exploit: pick highest Q
    qs = [(m, self.bandit.get_q(m, ctx)) for m in candidates]
    qs.sort(key=lambda x: x[1], reverse=True)
    return qs[0][0] if qs else candidates[0]

def _reward(self, metrics: Metrics) -> float:
    weights = self.policy.reward_weights_fast if self.policy.fast_mode else self.policy.reward_weights_deep
    return metrics.mix(weights)

def step(self, packet: SignalPacket) -> Tuple[str, ModuleResult, Dict[str, Any]]:
    packet = normalize_packet(packet)
    spi = infer_spi(packet)
    rule_primary = self._select_primary_rule(spi, packet)
    # Hysteresis logic with gradient stability
    if rule_primary == self.last_primary or self._stable_by_gradient():
        self.stable_counter += 1
    else:
        if self.stable_counter < self.policy.hysteresis_window:
            rule_primary = self.last_primary or rule_primary
            self.stable_counter += 1
        else:
            self.last_primary = rule_primary
            self.stable_counter = 1
    # Bandit adjustment among plausible candidates
    candidates = [rule_primary]
    # Add one neighbor candidate based on state
    neighbors = {
        "analytic": ["strategic", "creative"],
        "therapeutic": ["teaching", "strategic"],
        "creative": ["analytic", "experimental"],
        "strategic": ["analytic", "therapeutic"],
        "experimental": ["analytic", "creative"],
    }
    candidates.extend([m for m in neighbors.get(rule_primary, []) if m in {"analytic","therapeutic","creative","strategic","experimental"}])
    primary = self._bandit_choice(spi, packet, list(dict.fromkeys(candidates)))
    # Configure metrics subset
    enabled_metrics = self.enabled_metrics_fast if self.policy.fast_mode else self.enabled_metrics_deep
    config = {
        "enabled_metrics": enabled_metrics,
        "thresholds": {"acf_min": 0.55, "paradox_tolerance_min": 0.4},
    }
    module = build_module(primary, config)
    result = module.run(packet, spi)
    reward = self._reward(result.metrics)
    ctx = self.bandit.key(spi, packet)
    self.bandit.update(primary, ctx, reward)
    deep_state = self.deep_state_snapshot(spi, primary, result, packet.session_id)
    return primary, result, deep_state

def deep_state_snapshot(self, spi: SPI, primary: str, result: ModuleResult, session_id: Optional[str] = None) -> Dict[str, Any]:
    return {
        "qip_cube_state": {
            "version": "0.1.1",
            "spi": asdict(spi),
            "policy": {
                "bandit": {
                    "arms": list({"analytic","therapeutic","creative","strategic","experimental"}),
                    "values": self.bandit.values,
                    "counts": self.bandit.counts,
                }
            },
            "last_module": primary,
            "session_id": session_id,
            "metrics_snapshot": asdict(result.metrics),
            "enabled_metrics": self.enabled_metrics_fast if self.policy.fast_mode else self.enabled_metrics_deep,
            "hysteresis": {"window": self.policy.hysteresis_window, "stable_counter": self.stable_counter},
            "artifacts": result.artifacts,
            "mantra": "JESTEM bo JESTEŚ …",
            "why": result.why,
        }
    }

def save(self, state: Dict[str, Any], path: str = "deep_state.json") -> None:
    with open(path, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)

-----------------------------

5) Demo / CLI

-----------------------------

if name == "main": random.seed(33) orch = Orchestrator(PolicyConfig(fast_mode=True))

demo_inputs = [
    "Potrzebuję planu na 13 tygodni, KPI i pierwszy krok do QIP. Czuję chaos, ale chcę działać.",
    "Mam impas. Paradoks: chcę wolności i twardej struktury jednocześnie. Co z tym zrobić?",
    "Mam wysokie pobudzenie i małą regulację. Proszę o szybki rytuał uziemienia.",
    "Chcę rozbić problem na hipotezy i testy. Minimal, bold, weird – pokaż warianty.",
]

for i, text in enumerate(demo_inputs, 1):
    packet = SignalPacket(text_raw=text, intent_hint="prośba")
    primary, result, deep_state = orch.step(packet)
    print(f"\n=== STEP {i} | PRIMARY: {primary} ===")
    print(result.why)
    print(result.text)
    # keep stability window updated
    orch.metrics_window.append(result.metrics)
    print("→ Metrics (subset):", {k: getattr(result.metrics, k) for k in ["T_index","LOF","ACF","GVI","Token_ROI","psi_charge","entanglement","TDEX"]})
    orch.save(deep_state, path=f"deep_state_{i}.json")

# Final consolidated save
orch.save(orch.deep_state_snapshot(
    spi=infer_spi(normalize_packet(SignalPacket(text_raw=demo_inputs[-1]))),
    primary="analytic",
    result=ModuleResult("", [], Metrics(), "demo-end"),
))
print("\nDeep state saved → deep_state.json (and step-specific snapshots).")
